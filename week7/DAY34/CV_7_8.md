# (7강) Instance/Panoptic segmentation
### Instance segmentation
#### What is instance segmentation?
- Instance segmentation = Semantic segmentation + distinguishing instances
#### Instance segmenters
- Mask R-CNN
	- RoI extraction through RoIAlign, an improved version of RoI Pooling
	- Faster R-CNN + Mask branch
		- Binay classification for each class
- YOLACT ( You Only Look At CoefficienTs)
- YolactEdge

### Panoptic segmentation
#### What is panoptic segmentation?
- Semantic segmentation(Stuff + Things)
- Panoptic segmentation(Stuff + Instances of Things)
#### UPSNet & VPSNet
- UPSNet
	- Semantic & Instance head -> Panoptic head -> Panoptic logits
- VPSNet (for video)
	- 1. Align reference features onto the target feature map (Fusion at pixel level)
	- 2. Track module associates different object instances (Track at instance level)
	- 3. Fused-and-tracked modules are trained to synergize each other

### Landmark localization
#### What is landmark localization?
- Facial landmark localization
- Human pose estimation
- Landmark localization ( = keypoint estimation): Predicting the coordinates of keypoints.
#### Coordinate regression vs. heatmap classification
1) Coordinate regression: usually inaccurate and biased
2) Heatmap classification: better performance but high computational cost
#### Hourglass network
- Stacked hourglass modules allow for repeated bottom-up and top-down inference that refines the output of the previous hourglass module.
#### Extensions
- DensePose
	- All pixels -> 3D surface of the human body
	- uv map is a flattened representation of 3D geometry.<br> Also, UV map is invariant to motion (i.e., canonical coordinate)
	- DensePose R-CNN = Faster R-CNN + 3D surface regression branch
- RetinaFace
	- RetinaFace \ FPN + Multi-task branches
### Detecting objects as keypoints
#### CornerNet & CenterNet
- CornerNet
	- Bounding box = {Top-left, Bottom-right} corners
- CenterNet(1)
	- Bounding box = {Top-left, Bottom-right, Center} points
- CenterNet(2)
	- Bounding box = {Width, Height, Center} Points

# (8강) Conditional generative model
### Conditional generative model
#### Conditional generative model
- Translating an image given "condition"
	- We can explicitly generate an image corresponding to a given "condition"!
- Generative model vs. Conditional generative model
	- Generative model generates a random sample
	- Conditional generative model generates a random sample under the given "condition"
- Example of conditional generative model - audio super resolution
	- P(high resolution audio | low resolution audio)
#### Conditional GAN and image translation
- Image-to-Image translation
	- Translating an image into another image
	- Many applications: Style transfer, Super resolution, Colorization!
#### Example: Super resolution
- Example of conditional GAN - low resolution to high resolution
	- Input: Low resolution image
	- Output: High resolution image corresponding to the input image
	- An example of conditional GAN

### Image translation GANs
#### Pix2Pix
- Task definition
	- Translating an image to a corresponding image in another domain(e.g., style)
	- Example of a conditional GAN where the condition is given as an input image
- Loss function of Pix2Pix
	- If only using L1 loss, blurry images are generated
	- GAN(adversarial_ loss induces more realistic outputs close to real distribution
- Role of GAN loss in Pix2Pix
	- Pix2Pix generates realistic images by using both GAN loss and L1 loss
#### CycleGAN
- In Pix2Pix, we need "pairwise data" to learn translation between two domains(supervised)
	- E.g., "sketch" and "real image" pairs are required to translate a sketch into a real image
	- However, it is hard or impossible to get a pairwise dataset
	- CycleGAN enables the traslation between domains with non-pairwise datasets
	- Do not require direct correspondences between (Monet portrait, real photo)
- Loss function of CycleGAN
	- CycleGAN loss = GAN loss(in both direction) + cycle-consistency loss
	- GAN loss: Translate an image in domain A to B, and vice versa
	- Cycle-consistency loss: Enforce the fact that an image and its manipulated one by going back-and-forth should be same
- If we solely use GAN loss...
	- (Mode Collapse) Regardless of input, the generator could always output the same one!
	- Contents of input is not properly reflected in output!
- Solution: cycle-consistency loss to preserve contents
	- Translate an image in X to Y, and translate its output image into X again
		- The recovered image should be same with the original image!
	- Contents in an image should be preserved to do so
	- Not supervision(i.e., self-supervision)
#### Perceptual loss
- GAN is hard to train
	- GAN is hard to train(alternating training is required)
	- Is the another way to get high-quality image without GAN?
- Perceptual loss, yet another approach for achieving high quality output
- GAN loss
	- Relatively hard to train and code(Generator & Discriminator adversarially improve)
	- Do not require any pre-trained networks
	- Since no pre-trained network is required, can be applied to various applications
- Perceptual loss
	- Simple to train and code (trained only with simple forward & backward computation)
	- Requiring a pre-trained network to measure a learned loss
	- Observation: Pre-trained classifiers have filter responses similar to humans' visual perception
	- By utilizing such pre-trained "perception", we may transform an image to a perceptual space
	- Image Transform Net: Output a transformed image from input
	- Loss Network: Compute style and feature losses between a generated image and targets
		- Typically, the VGG model pre-trained on ImageNet is used
		- Fixed during training Image Trasform Net
- Feature reconstruction loss
	- The output image and target image are fed into the loss network
	- Compute L2 loss between the feature maps of output and target images
- Style reconstruction loss
	- Similarly, the output image and target image are fed into the loss network
	- Compute L2 loss between the gram matrices generated from the feature maps.
### Various GAN applications
- Deepfake
	- Converting human face of voice in video into another face or voice
- Ethical concerns about Deepfake
	- Fake video ( images ) can be easily generated using GAN
	- Preventing crimes using GAN is emerging as an important challenge
- Face de-identification
	- Protecting privacy by slightly modifying human face image
	- Results look similar to human bet hard for computer to identify them as same person