# (9강) Multi-modal: Captioning and speaking
### Overview of multi-modal learning
- Modalities in multi in multi-modal learning
- Challenge (1) - Different representations between modalities
- Challenge (2) - Unbalance between heterogeneous feature spaces
- Challenge (3) - May a model be biased on a specific modality
- Despite the challenges, multi-modal learning is fruitful and important

### Multi-modal tasks (1) - Visual data & Text
#### Text embedding
- Text embedding - Example
	- Characters are hard to use in machine learning
	- Map to dense vectors
	- Surprisingly, generalization power is obtained by learning dense representation
		- E.g., cat vs. kitten
- word2vec - Skip-gram model
	- Trained to learn W and W'
	- Rows in W represent word embedding vectors
	- Learning to predict neighboring N words for understanding relationships between words
		- Given a model with a window of size 5, the center words depend on 4 words
#### joint embedding
- Application - Image tagging
	- Can generate tags of a given image, and retrieve images by a tag keyword as well
	- Combining pre-trained unimodal models
	- Metric learning in visual-semantic space
	- Interesting property
		- Surprisingly, learned embeddings hold analogy relationships between visual and text data
- Application - Image & food recipe retrieval
#### Cross modal translation
- Application - Image captioning
- Captioning as image-to-sentence - CNN for image & RNN for sentence
- Show and tell
	- Encoder: CNN model pre-trained on ImageNet
	- Decoder: LSTM module
- Show, attend, and tell
- Text-to-image by generative model
#### Cross modal reasoning
- Visual question answering 
	- Multiple streams
	- Joint embedding
	- End-to-end training
### Multi-modal tasks (2) - Visual data & Audio
#### Sound representation
- Sound representation
	- Acoustic feature extraction from wave form to spectrogram
	- Fourier transform
		- Short-time Fourier transform(STFT): Fourier transform(FT) on windowed waveform results in frequency-magnitude graph
		- FT decomposes an input signal into constituent frequencies
	- Spectrogram
		- A stack of spectrums along the time axis
#### Joint embedding
- Application - Scene recognition by sound
- SoundNet
	- Learn audio representation from synchronized RGB frames in the same videos
	- Train by the teacher-student manner
		- Transfer visual knowledge from pre-trained visual recognition models into sound modality
	- For a target task, the pre-trained internal representation (pool5) is used as features
	- Training a classifier with the pool5 feature
		- Instead of the output layer, the pool5 feature posses more generalizable semantic info.
#### Cross modal translation
- Speech2Face
	- Training by feature matching loss(self-supervised manner) for making features compatible
		- Natural co-occurrence of speaker's speech and facial images
- Image-to-speech synthesis
#### Cross modal reasoning
- Sound source localization

# (10강) 3D understanding
### Seeing the world in 3D perspective
#### Why is 3D important?
- We live in a 3D space
	- AI agents operate in the real world, which is a 3D space
#### The way we observe 3D
- An image is a projection of the 3D world onto a 2D space.
- The camera is projection device of the 3D scene onto a 2D image plane
- Triangulation - The way to obtain a 3D point from 2D images
#### 3D data representation
- How is 3D data represented in computer?
	- A 2D image is represented by RGB values of each pixel in 2D array structure
- 3D data representation is not unique
	- Multi-view images
	- Volumetric(voxel)
	- part assembly
	- point cloud
	- mesh
	- Implicit shape
#### 3D datasets
- ShapeNet
	- Large scale synthetic objects(51,300 3D models with 55 categories)
- PartNet (shapeNetPart2019)
	- Fine-grained dataset, useful for segmentation(573,585 part instances in 26,671 3D models)
- SceneNet
	- 5 million RGB-Depth synthetic indoor images
- ScanNet
	- RGB-Depth dataset with 2.5 million views obtained from more than 1500 scans
### 3D tasks
#### 3D recognition
- 3D object recognition
	- Recognizing a 3D object like the object recognition in 2D image
#### 3D object detection
- 3D object detection
	- Detecting 3D object locations in image or 3D spaces
	- Useful for autonomous driving applications
#### 3D semantic segmentation
- 3D semantic segmentation
	- Semantic segmentation of 3D data, such as neuroimaging
#### Conditional 3D generation
- Mesh R-CNN
	- Input: a 2D image, output: 3D meshes of detected objects
	- Can be implemented by modification from Mask R-CNN
- Recap: Branches in Mask R-CNN
	- Mask R-CNN segments objects by predicting "box", "classes", and "mask"
	- Branches infer each output from a shared feature corresponding to each RoI
- Mask R-CNN vs. Mesh R-CNN
	- Mesh R-CNN: "3D branch" is added to Mask R-CNN
	- The 3D branch outputs a 3D mesh of an object
- More complex 3D reconstruction models
	- Decomposing 3D object reconstruction into multiple sub-problems
	- Sub-problems: physically meaningful disentanglement (Surface normal, depth, silhouette, ...)
### 3D application example
#### Photo refocusing
- Defocusing a photo using depth map
- Imaplement the post-refocusing feature in your phone
	- Implementing an after-refocusing image feature given depth map
	- Almost same with "portrait mode" in your smartphone camera app
	- Depth map can be estimated by depth sensors or neural networks
- Defocusing a photo using depth map
	- 1. Set a depth threshold range [D_min, D_max] you wnat to focus
	- 2. Compute a mask of "focusing area" and "defocusing area" by depth map thresholding
	- 3. Generate a blurred version of the input image
	- 4. Compute "Masked focused image" and "Masked defocused image"
	- 5. Blend masked images to generate a refocused image