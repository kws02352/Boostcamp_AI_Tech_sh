{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2_word2vec.ipynb의 사본","provenance":[{"file_id":"1VTvW_xhCVskuAJokXST6RImd8CAZ7Pdr","timestamp":1613518273952}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pTUhmMdzMPlN"},"source":["### CBOW(Continuous Bag-of-Words)\r\n","\r\n","- 주변 단어들을 가지고 중심 단어를 예측하는 방식으로 학습합니다.\r\n","- 주변 단어들의 one-hot encoding 벡터를 각각 embedding layer에 projection하여 각각의 embedding 벡터를 얻고 이 embedding들을 element-wise한 덧셈으로 합친 뒤, 다시 linear transformation하여 예측하고자 하는 중심 단어의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만든 뒤, 중심 단어의 one-hot encoding 벡터와의 loss를 계산합니다.\r\n","- 예) A cute puppy is walking in the park. & window size: 2\r\n","  - Input(주변 단어): \"A\", \"cute\", \"is\", \"walking\"\r\n","  - Output(중심 단어): \"puppy\""]},{"cell_type":"markdown","metadata":{"id":"WadptNGsMc8Z"},"source":["### Skip-gram\r\n","\r\n","- 중심 단어를 가지고 주변 단어들을 예측하는 방식으로 학습합니다.\r\n","- 중심 단어의 one-hot encoding 벡터를 embedding layer에 projection하여 해당 단어의 embedding 벡터를 얻고 이 벡터를 다시 linear transformation하여 예측하고자 하는 각각의 주변 단어들과의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만든 뒤, 그 주변 단어들의 one-hot encoding 벡터와의 loss를 각각 계산합니다.\r\n","- 예) A cute puppy is walking in the park. & window size: 2\r\n","  - Input(중심 단어): \"puppy\"\r\n","  - Output(주변 단어): \"A\", \"cute\", \"is\", \"walking\""]},{"cell_type":"markdown","metadata":{"id":"h3FAK0fz1kOr"},"source":["##**2. Word2Vec**\r\n","1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\r\n","2. CBOW, Skip-gram 모델을 각각 구현합니다.\r\n","3. 모델을 실제로 학습해보고 결과를 확인합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7F3RNtHL3Vt","executionInfo":{"status":"ok","timestamp":1613518787441,"user_tz":-540,"elapsed":707,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"55172060-c582-4def-cee0-243abe9670ff"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u9FrxTPWIsct"},"source":["### **필요 패키지 import**"]},{"cell_type":"code","metadata":{"id":"QjroCdtwI9Rz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613518792804,"user_tz":-540,"elapsed":2715,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"a7ef89f6-bc31-43c1-fc81-67f059081f85"},"source":["!pip install konlpy"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nSP7aXfJIr3i","executionInfo":{"status":"ok","timestamp":1613518800745,"user_tz":-540,"elapsed":4411,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["from tqdm import tqdm\r\n","from konlpy.tag import Okt\r\n","from torch import nn\r\n","from torch.nn import functional as F\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from collections import defaultdict\r\n","\r\n","import torch\r\n","import copy\r\n","import numpy as np"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qugro74yJASr"},"source":["### **데이터 전처리**"]},{"cell_type":"markdown","metadata":{"id":"Q36dfSRRJDtX"},"source":["\r\n","\r\n","데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  \r\n","학습 데이터는 1번 실습과 동일하고, 테스트를 위한 단어를 아래와 같이 가정해봅시다."]},{"cell_type":"code","metadata":{"id":"CLZ2f-lRJSus","executionInfo":{"status":"ok","timestamp":1613518897501,"user_tz":-540,"elapsed":617,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["train_data = [\r\n","  \"정말 맛있습니다. 추천합니다.\",\r\n","  \"기대했던 것보단 별로였네요.\",\r\n","  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\r\n","  \"완전 최고입니다! 재방문 의사 있습니다.\",\r\n","  \"음식도 서비스도 다 만족스러웠습니다.\",\r\n","  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\r\n","  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\r\n","  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\r\n","  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\r\n","  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \r\n","]\r\n","\r\n","test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vReElaFSLBYL"},"source":["Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."]},{"cell_type":"code","metadata":{"id":"dTjlRzmWMDK_","executionInfo":{"status":"ok","timestamp":1613518902138,"user_tz":-540,"elapsed":2124,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["tokenizer = Okt()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DTUsX672icp","executionInfo":{"status":"ok","timestamp":1613518903277,"user_tz":-540,"elapsed":619,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["def make_tokenized(data):\r\n","  tokenized = []\r\n","  for sent in tqdm(data):\r\n","    tokens = tokenizer.morphs(sent, stem=True)\r\n","    tokenized.append(tokens)\r\n","\r\n","  return tokenized"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-z0z6HD2rrX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613518913102,"user_tz":-540,"elapsed":5973,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"5397b7b9-93a9-4092-a849-2ef4deb9f07f"},"source":["train_tokenized = make_tokenized(train_data)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:05<00:00,  1.81it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"51exEpI0Mc3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613518915227,"user_tz":-540,"elapsed":634,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"e89f4ab1-d468-4dcc-f321-f6505fc9618c"},"source":["word_count = defaultdict(int)\r\n","\r\n","for tokens in tqdm(train_tokenized):\r\n","  for token in tokens:\r\n","    word_count[token] += 1"],"execution_count":10,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 13516.93it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gyvHAMAnMh1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613518919427,"user_tz":-540,"elapsed":609,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"873275eb-dd32-459a-9470-4ece3b448f72"},"source":["word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\r\n","print(list(word_count))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DaK_i3zL2vO3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613518921941,"user_tz":-540,"elapsed":589,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"a6469154-4a7c-4255-efb2-e07c351f35a3"},"source":["w2i = {}\r\n","for pair in tqdm(word_count):\r\n","  if pair[0] not in w2i:\r\n","    w2i[pair[0]] = len(w2i)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|██████████| 60/60 [00:00<00:00, 97202.87it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LiGqiEGDL5B_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613518924638,"user_tz":-540,"elapsed":631,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"ef8c351d-5c18-419b-999d-56d32c1ed4f7"},"source":["print(train_tokenized)\r\n","print(w2i)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n","{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vXA5zaPPM3Wd"},"source":["실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."]},{"cell_type":"code","metadata":{"id":"s47ssyVt89t1","executionInfo":{"status":"ok","timestamp":1613519337956,"user_tz":-540,"elapsed":617,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["class CBOWDataset(Dataset):\r\n","  def __init__(self, train_tokenized, window_size=2):\r\n","    self.x = []\r\n","    self.y = []\r\n","\r\n","    for tokens in tqdm(train_tokenized):\r\n","      token_ids = [w2i[token] for token in tokens]\r\n","      for i, id in enumerate(token_ids):\r\n","        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n","          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n","          self.y.append(id)\r\n","\r\n","    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\r\n","    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n","\r\n","  def __len__(self):\r\n","    return self.x.shape[0]\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return self.x[idx], self.y[idx]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvInhQ33AMJv","executionInfo":{"status":"ok","timestamp":1613519340053,"user_tz":-540,"elapsed":617,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["class SkipGramDataset(Dataset):\r\n","  def __init__(self, train_tokenized, window_size=2):\r\n","    self.x = []\r\n","    self.y = []\r\n","\r\n","    for tokens in tqdm(train_tokenized):\r\n","      token_ids = [w2i[token] for token in tokens]\r\n","      for i, id in enumerate(token_ids):\r\n","        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n","          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n","          self.x += [id] * 2 * window_size\r\n","\r\n","    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\r\n","    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n","\r\n","  def __len__(self):\r\n","    return self.x.shape[0]\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return self.x[idx], self.y[idx]"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyAGV5IUUba0"},"source":["각 모델에 맞는 `Dataset` 객체를 생성합니다."]},{"cell_type":"code","metadata":{"id":"5ep7Hm6oBWyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613519424082,"user_tz":-540,"elapsed":577,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"b0086ecd-f9c1-4506-b466-cf1a68d7330e"},"source":["cbow_set = CBOWDataset(train_tokenized)\r\n","skipgram_set = SkipGramDataset(train_tokenized)\r\n","print(list(skipgram_set))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 37249.59it/s]\n","100%|██████████| 10/10 [00:00<00:00, 19963.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1QSo73PoRyd9"},"source":["### **모델 Class 구현**"]},{"cell_type":"markdown","metadata":{"id":"jnnk44R6R28x"},"source":["차례대로 두 가지 Word2Vec 모델을 구현합니다.  \r\n","\r\n","\r\n","*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\r\n","*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\r\n"]},{"cell_type":"code","metadata":{"id":"b_HP1ISq5CWv","executionInfo":{"status":"ok","timestamp":1613519547597,"user_tz":-540,"elapsed":592,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["class CBOW(nn.Module):\r\n","  def __init__(self, vocab_size, dim):\r\n","    super(CBOW, self).__init__()\r\n","    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n","    self.linear = nn.Linear(dim, vocab_size)\r\n","\r\n","  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n","  def forward(self, x):  # x: (B, 2W)\r\n","    embeddings = self.embedding(x)  # (B, 2W, d_w)\r\n","    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\r\n","    output = self.linear(embeddings)  # (B, V)\r\n","    return output"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQAUApww68MJ","executionInfo":{"status":"ok","timestamp":1613519549459,"user_tz":-540,"elapsed":633,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["class SkipGram(nn.Module):\r\n","  def __init__(self, vocab_size, dim):\r\n","    super(SkipGram, self).__init__()\r\n","    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n","    self.linear = nn.Linear(dim, vocab_size)\r\n","\r\n","  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n","  def forward(self, x): # x: (B)\r\n","    embeddings = self.embedding(x)  # (B, d_w)\r\n","    output = self.linear(embeddings)  # (B, V)\r\n","    return output"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58cJalkDWYMT"},"source":["두 가지 모델을 생성합니다."]},{"cell_type":"code","metadata":{"id":"8vWUXEi8WeM-","executionInfo":{"status":"ok","timestamp":1613519557212,"user_tz":-540,"elapsed":634,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["cbow = CBOW(vocab_size=len(w2i), dim=256)\r\n","skipgram = SkipGram(vocab_size=len(w2i), dim=256)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxP7qdtNWil1"},"source":["### **모델 학습**"]},{"cell_type":"markdown","metadata":{"id":"QVggZrQ4WpBS"},"source":["다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."]},{"cell_type":"code","metadata":{"id":"ygVdz5rSBeNu","executionInfo":{"status":"ok","timestamp":1613519561333,"user_tz":-540,"elapsed":650,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}}},"source":["batch_size=4\r\n","learning_rate = 5e-4\r\n","num_epochs = 5\r\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n","\r\n","cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\r\n","skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekixqKB3X5C1"},"source":["첫번째로 CBOW 모델 학습입니다."]},{"cell_type":"code","metadata":{"id":"-d95qR7oC822","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613519574698,"user_tz":-540,"elapsed":11071,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"c43e77a0-21ff-47f5-ab97-97b7a171c955"},"source":["cbow.train()\r\n","cbow = cbow.to(device)\r\n","optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\r\n","loss_function = nn.CrossEntropyLoss()\r\n","\r\n","for e in range(1, num_epochs+1):\r\n","  print(\"#\" * 50)\r\n","  print(f\"Epoch: {e}\")\r\n","  for batch in tqdm(cbow_loader):\r\n","    x, y = batch\r\n","    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n","    output = cbow(x)  # (B, V)\r\n"," \r\n","    optim.zero_grad()\r\n","    loss = loss_function(output, y)\r\n","    loss.backward()\r\n","    optim.step()\r\n","\r\n","    print(f\"Train loss: {loss.item()}\")\r\n","\r\n","print(\"Finished.\")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 82.78it/s]\n","  0%|          | 0/16 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 1\n","Train loss: 5.32407283782959\n","Train loss: 3.5595760345458984\n","Train loss: 4.547629356384277\n","Train loss: 3.913658380508423\n","Train loss: 4.479578018188477\n","Train loss: 4.342905044555664\n","Train loss: 4.667276382446289\n","Train loss: 5.308061599731445\n","Train loss: 4.54544734954834\n","Train loss: 4.497493743896484\n","Train loss: 4.771795272827148\n","Train loss: 4.435040473937988\n","Train loss: 3.8879635334014893\n","Train loss: 4.878297805786133\n","Train loss: 5.111123085021973\n","Train loss: 3.9713449478149414\n","##################################################\n","Epoch: 2\n","Train loss: 5.13508415222168\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 616.28it/s]\n","100%|██████████| 16/16 [00:00<00:00, 631.56it/s]\n","  0%|          | 0/16 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 3.434051990509033\n","Train loss: 4.425656795501709\n","Train loss: 3.808619499206543\n","Train loss: 4.3524346351623535\n","Train loss: 4.063481330871582\n","Train loss: 4.488699436187744\n","Train loss: 5.173338890075684\n","Train loss: 4.425926208496094\n","Train loss: 4.331645488739014\n","Train loss: 4.580134868621826\n","Train loss: 4.057640552520752\n","Train loss: 3.766841411590576\n","Train loss: 4.770680904388428\n","Train loss: 4.937209129333496\n","Train loss: 3.846787452697754\n","##################################################\n","Epoch: 3\n","Train loss: 4.948372840881348\n","Train loss: 3.311044931411743\n","Train loss: 4.306021690368652\n","Train loss: 3.7045955657958984\n","Train loss: 4.22697639465332\n","Train loss: 3.8030028343200684\n","Train loss: 4.312788963317871\n","Train loss: 5.040936470031738\n","Train loss: 4.311140060424805\n","Train loss: 4.172423839569092\n","Train loss: 4.3983259201049805\n","Train loss: 3.7057337760925293\n","Train loss: 3.647672414779663\n","Train loss: 4.6649017333984375\n","Train loss: 4.767360687255859\n","Train loss: 3.7263190746307373\n","##################################################\n","Epoch: 4\n","Train loss: 4.764030456542969\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 55.27it/s]\n","100%|██████████| 16/16 [00:00<00:00, 670.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 3.1906325817108154\n","Train loss: 4.188723087310791\n","Train loss: 3.6016454696655273\n","Train loss: 4.103211879730225\n","Train loss: 3.561896324157715\n","Train loss: 4.139711856842041\n","Train loss: 4.910799980163574\n","Train loss: 4.200994968414307\n","Train loss: 4.020262718200684\n","Train loss: 4.226686477661133\n","Train loss: 3.381533145904541\n","Train loss: 3.530536413192749\n","Train loss: 4.560973167419434\n","Train loss: 4.601377010345459\n","Train loss: 3.6096858978271484\n","##################################################\n","Epoch: 5\n","Train loss: 4.582165718078613\n","Train loss: 3.072896718978882\n","Train loss: 4.0737624168396\n","Train loss: 3.4998278617858887\n","Train loss: 3.9811534881591797\n","Train loss: 3.340017318725586\n","Train loss: 3.969665050506592\n","Train loss: 4.782879829406738\n","Train loss: 4.0954179763793945\n","Train loss: 3.875405788421631\n","Train loss: 4.065690517425537\n","Train loss: 3.087124824523926\n","Train loss: 3.4155163764953613\n","Train loss: 4.45890998840332\n","Train loss: 4.439147472381592\n","Train loss: 3.4966869354248047\n","Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FDahBf6IX4py"},"source":["다음으로 Skip-gram 모델 학습입니다."]},{"cell_type":"code","metadata":{"id":"jJxGEusqFV5r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613519610604,"user_tz":-540,"elapsed":1059,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"1d8a04d0-41da-4c7f-8ccf-bf8f713536c5"},"source":["skipgram.train()\r\n","skipgram = skipgram.to(device)\r\n","optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\r\n","loss_function = nn.CrossEntropyLoss()\r\n","\r\n","for e in range(1, num_epochs+1):\r\n","  print(\"#\" * 50)\r\n","  print(f\"Epoch: {e}\")\r\n","  for batch in tqdm(skipgram_loader):\r\n","    x, y = batch\r\n","    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n","    output = skipgram(x)  # (B, V)\r\n","\r\n","    optim.zero_grad()\r\n","    loss = loss_function(output, y)\r\n","    loss.backward()\r\n","    optim.step()\r\n","\r\n","    print(f\"Train loss: {loss.item()}\")\r\n","\r\n","print(\"Finished.\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["100%|██████████| 64/64 [00:00<00:00, 720.61it/s]\n","100%|██████████| 64/64 [00:00<00:00, 725.44it/s]\n","  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 1\n","Train loss: 4.501903533935547\n","Train loss: 3.8635504245758057\n","Train loss: 4.141275405883789\n","Train loss: 4.496247291564941\n","Train loss: 4.316728115081787\n","Train loss: 4.261194229125977\n","Train loss: 4.368272304534912\n","Train loss: 4.098947525024414\n","Train loss: 4.2619476318359375\n","Train loss: 4.528131008148193\n","Train loss: 3.8184611797332764\n","Train loss: 4.191086769104004\n","Train loss: 4.1431474685668945\n","Train loss: 4.233884334564209\n","Train loss: 4.2071638107299805\n","Train loss: 4.326584815979004\n","Train loss: 4.152448654174805\n","Train loss: 4.223211288452148\n","Train loss: 4.215305328369141\n","Train loss: 4.092764854431152\n","Train loss: 4.458646297454834\n","Train loss: 4.170133113861084\n","Train loss: 4.835692405700684\n","Train loss: 4.005480766296387\n","Train loss: 4.282678127288818\n","Train loss: 4.412858963012695\n","Train loss: 4.538447380065918\n","Train loss: 4.131497383117676\n","Train loss: 4.506120204925537\n","Train loss: 4.707592487335205\n","Train loss: 4.128485202789307\n","Train loss: 4.548283576965332\n","Train loss: 4.213082790374756\n","Train loss: 4.254568576812744\n","Train loss: 4.134583473205566\n","Train loss: 3.8743038177490234\n","Train loss: 4.11086368560791\n","Train loss: 4.324228286743164\n","Train loss: 4.148399829864502\n","Train loss: 4.032462120056152\n","Train loss: 4.915779113769531\n","Train loss: 3.8103866577148438\n","Train loss: 4.50728702545166\n","Train loss: 3.9035568237304688\n","Train loss: 4.09478759765625\n","Train loss: 4.57551908493042\n","Train loss: 4.34470272064209\n","Train loss: 4.794336318969727\n","Train loss: 4.184076309204102\n","Train loss: 4.222670555114746\n","Train loss: 3.9737744331359863\n","Train loss: 3.9878172874450684\n","Train loss: 3.5797505378723145\n","Train loss: 4.16068696975708\n","Train loss: 4.029505729675293\n","Train loss: 4.422371864318848\n","Train loss: 4.6488037109375\n","Train loss: 4.154877662658691\n","Train loss: 4.310315132141113\n","Train loss: 3.6472344398498535\n","Train loss: 3.993199586868286\n","Train loss: 4.413669586181641\n","Train loss: 4.033241271972656\n","Train loss: 3.896136999130249\n","##################################################\n","Epoch: 2\n","Train loss: 4.476934432983398\n","Train loss: 3.8311715126037598\n","Train loss: 4.105918884277344\n","Train loss: 4.431519508361816\n","Train loss: 4.2905120849609375\n","Train loss: 4.218118667602539\n","Train loss: 4.33098030090332\n","Train loss: 4.05632209777832\n","Train loss: 4.231408596038818\n","Train loss: 4.495551109313965\n","Train loss: 3.7827491760253906\n","Train loss: 4.161144256591797\n","Train loss: 4.109977722167969\n","Train loss: 4.1986918449401855\n","Train loss: 4.173800945281982\n","Train loss: 4.296934127807617\n","Train loss: 4.120172500610352\n","Train loss: 4.190727710723877\n","Train loss: 4.186861038208008\n","Train loss: 4.064579010009766\n","Train loss: 4.359148979187012\n","Train loss: 4.090322971343994\n","Train loss: 4.7741804122924805\n","Train loss: 3.966362953186035\n","Train loss: 4.237311363220215\n","Train loss: 4.347873687744141\n","Train loss: 4.494610786437988\n","Train loss: 4.104443550109863\n","Train loss: 4.462278366088867\n","Train loss: 4.677357196807861\n","Train loss: 4.104493141174316\n","Train loss: 4.5168914794921875\n","Train loss: 4.183595180511475\n","Train loss: 4.2243475914001465\n","Train loss: 4.1052985191345215\n","Train loss: 3.8429346084594727\n","Train loss: 4.053179740905762\n","Train loss: 4.282122611999512\n","Train loss: 4.111771583557129\n","Train loss: 4.002684593200684\n","Train loss: 4.872480392456055\n","Train loss: 3.789921283721924\n","Train loss: 4.445014953613281\n","Train loss: 3.8552842140197754\n","Train loss: 3.988473415374756\n","Train loss: 4.475735187530518\n","Train loss: 4.265313148498535\n","Train loss: 4.72947883605957\n","Train loss: 4.149971961975098\n","Train loss: 4.197447299957275\n","Train loss: 3.9355010986328125\n","Train loss: 3.9433772563934326\n","Train loss: 3.5525741577148438\n","Train loss: 4.13700008392334\n","Train loss: 3.996417999267578\n","Train loss: 4.390402793884277\n","Train loss: 4.5817155838012695\n","Train loss: 4.122553825378418\n","Train loss: 4.273221015930176\n","Train loss: 3.6170763969421387\n","Train loss: 3.960548162460327\n","Train loss: 4.370534896850586\n","Train loss: 4.010464668273926\n","Train loss: 3.852027416229248\n","##################################################\n","Epoch: 3\n","Train loss: 4.452365875244141\n","Train loss: 3.7989535331726074\n","Train loss: 4.070791721343994\n","Train loss: 4.367569923400879\n","Train loss: 4.264398097991943\n","Train loss: 4.17585563659668\n","Train loss: 4.294200420379639\n","Train loss: 4.014047622680664\n","Train loss: 4.201025009155273\n","Train loss: 4.463104724884033\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:00<00:00, 735.38it/s]\n","100%|██████████| 64/64 [00:00<00:00, 731.86it/s]\n","  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 3.7473230361938477\n","Train loss: 4.131361484527588\n","Train loss: 4.077559947967529\n","Train loss: 4.163684844970703\n","Train loss: 4.140625953674316\n","Train loss: 4.2675628662109375\n","Train loss: 4.088098049163818\n","Train loss: 4.158448219299316\n","Train loss: 4.158718109130859\n","Train loss: 4.036550521850586\n","Train loss: 4.260710716247559\n","Train loss: 4.012176513671875\n","Train loss: 4.713059425354004\n","Train loss: 3.927565813064575\n","Train loss: 4.192531108856201\n","Train loss: 4.283690452575684\n","Train loss: 4.451066970825195\n","Train loss: 4.077882289886475\n","Train loss: 4.418991565704346\n","Train loss: 4.647434234619141\n","Train loss: 4.080622673034668\n","Train loss: 4.4856343269348145\n","Train loss: 4.154275894165039\n","Train loss: 4.194477558135986\n","Train loss: 4.0761213302612305\n","Train loss: 3.8117876052856445\n","Train loss: 3.996396064758301\n","Train loss: 4.241329669952393\n","Train loss: 4.0757598876953125\n","Train loss: 3.9732844829559326\n","Train loss: 4.829377174377441\n","Train loss: 3.76967191696167\n","Train loss: 4.384131908416748\n","Train loss: 3.8073620796203613\n","Train loss: 3.884187936782837\n","Train loss: 4.37706184387207\n","Train loss: 4.187634468078613\n","Train loss: 4.66506290435791\n","Train loss: 4.116102695465088\n","Train loss: 4.172482490539551\n","Train loss: 3.8980631828308105\n","Train loss: 3.899585247039795\n","Train loss: 3.525634527206421\n","Train loss: 4.113747596740723\n","Train loss: 3.9635777473449707\n","Train loss: 4.358551979064941\n","Train loss: 4.5154128074646\n","Train loss: 4.090641975402832\n","Train loss: 4.236515998840332\n","Train loss: 3.587167501449585\n","Train loss: 3.928150177001953\n","Train loss: 4.327838897705078\n","Train loss: 3.988145351409912\n","Train loss: 3.8084516525268555\n","##################################################\n","Epoch: 4\n","Train loss: 4.428192138671875\n","Train loss: 3.766899824142456\n","Train loss: 4.035900115966797\n","Train loss: 4.304425239562988\n","Train loss: 4.2383880615234375\n","Train loss: 4.134398937225342\n","Train loss: 4.257935523986816\n","Train loss: 3.9721317291259766\n","Train loss: 4.170799255371094\n","Train loss: 4.430793762207031\n","Train loss: 3.7121853828430176\n","Train loss: 4.101740837097168\n","Train loss: 4.045880317687988\n","Train loss: 4.128867149353027\n","Train loss: 4.10764217376709\n","Train loss: 4.238473892211914\n","Train loss: 4.056230545043945\n","Train loss: 4.1263747215271\n","Train loss: 4.130877494812012\n","Train loss: 4.00868034362793\n","Train loss: 4.163442611694336\n","Train loss: 3.9357988834381104\n","Train loss: 4.652344226837158\n","Train loss: 3.889094591140747\n","Train loss: 4.148355484008789\n","Train loss: 4.220338344573975\n","Train loss: 4.407821178436279\n","Train loss: 4.051808834075928\n","Train loss: 4.376276016235352\n","Train loss: 4.617825031280518\n","Train loss: 4.056874752044678\n","Train loss: 4.4545135498046875\n","Train loss: 4.125125885009766\n","Train loss: 4.164960861206055\n","Train loss: 4.047048568725586\n","Train loss: 3.7808642387390137\n","Train loss: 3.9405922889709473\n","Train loss: 4.201907634735107\n","Train loss: 4.040370941162109\n","Train loss: 3.9442622661590576\n","Train loss: 4.7864766120910645\n","Train loss: 3.749634027481079\n","Train loss: 4.324718952178955\n","Train loss: 3.7598018646240234\n","Train loss: 3.78208065032959\n","Train loss: 4.279619216918945\n","Train loss: 4.1117753982543945\n","Train loss: 4.601105213165283\n","Train loss: 4.08247184753418\n","Train loss: 4.147773742675781\n","Train loss: 3.8614501953125\n","Train loss: 3.8564484119415283\n","Train loss: 3.498933792114258\n","Train loss: 4.09092378616333\n","Train loss: 3.9309864044189453\n","Train loss: 4.3268208503723145\n","Train loss: 4.449924945831299\n","Train loss: 4.059145927429199\n","Train loss: 4.2002034187316895\n","Train loss: 3.5575103759765625\n","Train loss: 3.896010160446167\n","Train loss: 4.285590648651123\n","Train loss: 3.966277599334717\n","Train loss: 3.7654218673706055\n","##################################################\n","Epoch: 5\n","Train loss: 4.404406547546387\n","Train loss: 3.7350122928619385\n","Train loss: 4.001249313354492\n","Train loss: 4.2421159744262695\n","Train loss: 4.212482452392578\n","Train loss: 4.093741416931152\n","Train loss: 4.222188472747803\n","Train loss: 3.93058180809021\n","Train loss: 4.140730381011963\n","Train loss: 4.398621082305908\n","Train loss: 3.677340507507324\n","Train loss: 4.0722856521606445\n","Train loss: 4.014925956726074\n","Train loss: 4.094242095947266\n","Train loss: 4.074853897094727\n","Train loss: 4.2096686363220215\n","Train loss: 4.02457332611084\n","Train loss: 4.094508171081543\n","Train loss: 4.103337287902832\n","Train loss: 3.980971097946167\n","Train loss: 4.067468166351318\n","Train loss: 3.8612990379333496\n","Train loss: 4.592050552368164\n","Train loss: 3.8509554862976074\n","Train loss: 4.10480260848999\n","Train loss: 4.157849311828613\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|██████████| 64/64 [00:00<00:00, 723.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.364875316619873\n","Train loss: 4.026218891143799\n","Train loss: 4.334148406982422\n","Train loss: 4.588531494140625\n","Train loss: 4.033250331878662\n","Train loss: 4.423529148101807\n","Train loss: 4.096147537231445\n","Train loss: 4.135798454284668\n","Train loss: 4.018080234527588\n","Train loss: 3.750169277191162\n","Train loss: 3.8858566284179688\n","Train loss: 4.163915634155273\n","Train loss: 4.005611896514893\n","Train loss: 3.9156172275543213\n","Train loss: 4.743786811828613\n","Train loss: 3.729802370071411\n","Train loss: 4.266858100891113\n","Train loss: 3.712614059448242\n","Train loss: 3.682309150695801\n","Train loss: 4.183540344238281\n","Train loss: 4.037846565246582\n","Train loss: 4.537623405456543\n","Train loss: 4.0490827560424805\n","Train loss: 4.123320579528809\n","Train loss: 3.825652837753296\n","Train loss: 3.8139736652374268\n","Train loss: 3.47247314453125\n","Train loss: 4.06852388381958\n","Train loss: 3.898646831512451\n","Train loss: 4.29521369934082\n","Train loss: 4.3852858543396\n","Train loss: 4.028070449829102\n","Train loss: 4.1642889976501465\n","Train loss: 3.528108596801758\n","Train loss: 3.864131212234497\n","Train loss: 4.243799209594727\n","Train loss: 3.9448559284210205\n","Train loss: 3.722949981689453\n","Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Pi0sbHV6dEOR"},"source":["### **테스트**"]},{"cell_type":"markdown","metadata":{"id":"WGarLWxXeJvz"},"source":["학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."]},{"cell_type":"code","metadata":{"id":"4A1wrl-L_RjF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613519615572,"user_tz":-540,"elapsed":631,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"1045531f-bca1-46ab-b84c-51723e7e63b3"},"source":["for word in test_words:\r\n","  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n","  emb = cbow.embedding(input_id)\r\n","\r\n","  print(f\"Word: {word}\")\r\n","  print(emb.squeeze(0))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Word: 음식\n","tensor([ 2.1512e-01,  1.6459e+00, -4.5536e-02,  9.7926e-02, -5.3946e-01,\n","         1.3987e-01, -7.0558e-01,  1.5987e+00, -6.7485e-01, -5.0270e-01,\n","         3.2398e-02,  1.5469e+00, -6.8049e-01, -6.7504e-02, -9.9103e-02,\n","        -2.6982e-01,  6.8192e-01, -7.6911e-02, -5.3510e-01,  8.4853e-01,\n","        -6.4414e-01, -6.9495e-01, -2.3576e+00, -1.9592e+00, -1.0910e+00,\n","        -7.1867e-01, -4.6458e-01,  1.2677e+00,  3.3500e-01,  1.0732e+00,\n","        -1.7693e+00, -7.0309e-01,  7.8530e-01,  5.1830e-01,  1.4247e-01,\n","        -2.3663e-01, -1.3117e+00,  2.2243e-01, -1.4052e+00,  2.1612e+00,\n","         2.4065e+00,  3.4722e-01, -7.3143e-01, -5.9588e-01, -1.2722e+00,\n","        -4.9789e-01,  8.7165e-01, -1.4515e-01,  2.5813e-01,  3.6628e-01,\n","         1.5090e+00,  7.1159e-01, -1.2334e+00,  2.0455e+00, -7.1456e-01,\n","         2.8921e-01, -7.4926e-01, -7.6920e-01, -2.3242e-01, -5.0443e-01,\n","        -9.7596e-01, -9.8172e-01,  1.9162e-02,  1.3211e+00, -2.1197e-01,\n","         9.3251e-01,  1.1003e+00, -1.2776e+00, -3.9951e-01, -1.0697e-01,\n","        -8.7743e-01,  1.4952e-01,  5.4928e-01,  1.1406e+00,  4.5048e-01,\n","         1.3915e+00,  7.5793e-01,  1.1918e+00, -4.7584e-01, -1.8133e-01,\n","         8.4029e-01, -3.8234e-01, -4.4876e-01, -6.4483e-01, -1.8751e+00,\n","        -2.3523e-01,  1.3549e-01,  1.0300e+00, -2.2456e+00,  1.6482e-01,\n","         1.4758e+00,  1.6506e+00,  1.9149e+00, -2.8604e+00, -1.5969e-01,\n","        -1.4966e+00, -3.7500e-01,  3.7414e-01,  7.2195e-01,  1.7993e+00,\n","         3.0279e-01,  3.0373e-02, -1.6343e+00, -1.0703e-01, -1.4779e+00,\n","        -1.3602e-01,  1.1198e+00,  6.1224e-01,  7.7326e-01,  6.5740e-01,\n","        -6.6778e-01, -3.6598e-01,  1.5978e-02, -1.2580e+00,  1.2083e+00,\n","         1.1628e+00, -8.2757e-01,  1.1002e+00, -1.2309e+00,  2.7648e-01,\n","        -1.0969e+00,  1.1112e+00,  2.3410e-01, -1.8719e-02, -9.7820e-01,\n","         1.4724e-01,  1.0093e+00, -7.3738e-01, -5.1156e-02, -1.3750e+00,\n","        -1.7317e+00,  4.0134e-01,  5.3009e-01, -2.9478e-01, -2.3574e-02,\n","         8.3853e-01, -8.8389e-01,  3.1124e-03, -3.7757e-02, -5.0708e-01,\n","        -5.7553e-01, -1.4386e-02, -9.5715e-01,  3.8864e-01, -5.0332e-01,\n","         6.9558e-01,  5.8239e-01, -1.2944e+00, -8.3375e-01, -1.6658e+00,\n","         1.1261e-01,  1.4125e-01,  1.1563e+00,  7.5714e-01,  5.0921e-01,\n","        -1.3871e-01, -3.5844e-01, -9.5522e-01, -1.8534e-01,  2.1185e-01,\n","        -9.8503e-02,  7.8276e-01, -8.3898e-01,  1.8149e-01,  1.0796e-01,\n","         6.2617e-01, -2.1764e-01,  3.4080e-01,  1.4534e+00,  4.2004e-01,\n","        -1.2386e+00,  2.2460e+00, -5.7335e-01, -6.5979e-01,  7.1391e-01,\n","         7.3972e-03,  5.0550e-01, -1.5322e+00,  1.5907e+00,  1.8133e-01,\n","        -6.6545e-01,  3.9228e-01, -1.5378e+00, -4.0052e-01,  1.3158e+00,\n","        -5.8735e-01, -6.8894e-01,  2.1169e+00, -5.8313e-01,  1.5817e+00,\n","        -3.2048e-01,  7.2588e-01,  3.9008e-02, -1.2354e+00,  5.6429e-01,\n","         9.4317e-02,  8.8023e-01,  7.6863e-02, -3.0126e-01,  2.0779e-01,\n","        -5.0688e-03, -1.8354e+00, -3.8520e-01,  4.2719e-01, -1.3195e+00,\n","        -5.5112e-01,  1.6745e-01, -4.4902e-01,  1.8756e+00,  3.2326e-01,\n","        -1.1137e+00, -2.3230e-01,  9.2070e-01,  1.2852e+00,  1.4639e+00,\n","         1.0974e+00,  1.3617e-01, -1.5070e-01, -6.9770e-01, -2.0558e+00,\n","        -1.5519e+00, -4.0537e-01,  1.7938e-01, -2.8777e-02, -1.6744e-01,\n","        -7.6230e-01, -1.0249e+00,  2.5812e+00,  9.2169e-01, -5.0365e-01,\n","        -2.2282e-01, -1.9554e+00,  5.9714e-01,  8.5726e-01,  3.9939e-01,\n","        -2.1031e+00, -3.0998e-01,  4.0066e-01, -3.8949e-01,  1.3884e+00,\n","        -9.8111e-01, -3.2894e-01, -5.5595e-01, -1.0704e-01,  2.1173e-02,\n","        -1.8245e+00, -1.0785e+00,  1.0584e-01, -2.9868e-01,  4.6850e-01,\n","         3.1442e-01,  6.4855e-01, -4.1742e+00, -1.9929e-01,  2.0970e-01,\n","         9.0002e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 맛\n","tensor([-1.4038, -0.3752, -0.9708, -0.1553, -1.0337, -1.8539, -0.6040, -1.3388,\n","         0.6784, -0.5052,  0.1071, -0.8387,  0.3807,  0.3947,  0.1794,  1.1663,\n","        -0.1933, -0.6934, -2.0785, -0.4118, -0.8977, -1.2239,  0.0913,  0.6489,\n","        -0.2105,  0.0611, -0.5141, -1.3303,  0.4790, -0.3414,  0.7782, -2.0980,\n","        -1.0088, -0.0065,  0.5417, -0.0764, -0.1911,  0.4835, -2.2475, -1.0984,\n","        -0.9097, -0.4970,  1.4382, -1.3744, -1.2429, -1.0869,  1.5086,  0.9239,\n","         0.9003, -0.8877, -1.5068, -2.5274,  2.6345,  0.7884, -0.5971,  1.5927,\n","        -0.4434,  0.3950,  0.4972, -0.6218,  0.1659,  1.1023, -1.8143, -0.8124,\n","        -1.0141,  0.6746, -0.8654,  0.8663, -0.5234, -1.9095, -0.6588,  1.7331,\n","         0.1216, -0.2390,  0.3389,  1.0546, -0.9906,  0.4554, -0.1177, -1.6504,\n","         0.0574, -0.8309,  3.0943, -0.4914, -0.2415, -0.1593,  2.4969,  0.0943,\n","        -1.1923,  0.3037,  0.6268, -1.0760, -0.1422,  0.1338, -0.5249, -2.2641,\n","         0.7624,  1.2045, -0.8023, -1.1383,  1.9185, -0.0576, -0.1290, -0.4754,\n","         1.2294,  0.1263,  0.3001,  0.9758,  0.2799,  0.9814, -1.5682,  0.0588,\n","         1.1064, -0.3372, -0.5247, -0.1504, -1.1185, -1.8364,  1.2368,  1.1518,\n","        -0.8614,  0.2259,  0.4445,  1.3432, -0.0042, -0.2145, -1.5428, -0.6662,\n","         1.2570, -0.5635,  0.7838,  0.7342,  0.2318, -1.8305, -0.5465, -0.3416,\n","        -0.9486,  0.5055,  0.6376, -0.8372,  1.5953,  0.2216,  0.1299,  0.1122,\n","        -0.0303, -0.3914, -1.3976,  0.0711, -1.6644, -0.1322,  1.7131,  0.2550,\n","        -0.7050,  1.4387,  0.6705, -1.6612,  2.1052,  1.8627, -0.3162, -0.8822,\n","         0.3801, -1.2544,  0.0334,  0.2328, -1.1000, -1.1537, -0.1993,  1.0152,\n","         0.6548,  0.5719,  0.2735,  0.3817, -2.4070, -0.5186,  0.0193, -0.5388,\n","         0.3392,  0.3968,  1.0283, -1.4449, -0.8427, -1.7031, -0.3482, -1.7436,\n","         0.0903,  0.3653,  0.6227,  0.2732, -0.7500,  1.8249,  0.7938, -1.4579,\n","        -0.7366, -0.5369,  0.0247, -0.3759, -2.1909, -1.8871, -0.2561,  0.3185,\n","        -0.2754,  0.4973, -1.3149,  1.6231,  1.0381,  0.7844, -0.5886,  2.0824,\n","        -0.4269,  0.3000, -2.8466, -0.0664, -0.2196,  0.2640,  1.6159,  0.6910,\n","        -0.1574, -1.4665, -0.3739,  1.1061, -0.5952, -1.3675,  0.6359, -1.1326,\n","         0.7418, -1.4067, -0.8623, -0.0690,  0.1209,  0.9566,  0.4011, -0.4090,\n","        -2.3684, -0.9752, -0.4119, -0.6493,  0.2863,  1.6390,  1.0904,  0.1839,\n","        -0.9129,  0.5848,  0.4688, -0.1497,  0.2944,  0.2420,  0.9548, -1.5393,\n","         1.3738, -0.6198, -0.5207, -0.7056, -1.9947,  0.4068, -1.0784,  0.6917],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 서비스\n","tensor([-1.1764,  1.1519, -0.2057, -0.4082, -0.0297,  1.9719, -0.7468, -0.8707,\n","         2.1471,  1.5095,  0.0601, -0.2970, -0.1618, -1.7331,  0.3111, -0.6694,\n","        -1.0507,  0.6911, -0.9885, -0.0411, -0.5492, -1.5576,  0.5343, -0.7841,\n","        -0.4283,  0.7870,  0.9889, -2.0764,  0.7778,  0.8705,  0.2657,  1.6247,\n","         0.8329,  1.1200, -0.6473,  0.6797, -0.1666, -0.4940,  1.6281, -0.1487,\n","        -0.3211, -0.2853,  1.3989,  0.2378, -0.0716,  0.8298, -1.0160,  3.0259,\n","        -2.1403, -0.3050,  0.2593, -0.4268,  1.4685, -0.9930,  0.2281, -0.3178,\n","         1.6669,  1.5607, -0.0881,  0.0057,  1.4944,  1.2473, -0.1676, -0.3535,\n","        -0.2263,  1.2568, -1.4155, -2.1988, -0.4364,  0.4571, -1.4044,  0.3549,\n","        -1.4058,  0.6582,  0.8024,  1.0892,  1.0714, -1.0844, -0.3144, -0.6524,\n","        -1.1069, -0.4968, -1.2726,  0.6991,  0.8851, -1.2905,  0.0579,  0.5509,\n","         1.2094, -1.1826, -0.0353, -0.8574,  0.6275,  0.2040,  0.0244,  0.2588,\n","        -2.9315, -0.1414,  0.8374, -0.9788, -1.5772, -0.5772, -1.0337, -0.0272,\n","         0.7995, -0.1971, -0.1686,  0.6621,  1.2660, -1.3582,  0.4449, -0.2256,\n","        -0.0113,  0.8487,  0.2334, -0.6038,  0.8710,  1.6794,  0.3175, -0.1383,\n","         1.1946,  0.8472, -1.2930,  1.0286, -0.3285,  1.7647, -1.2858, -1.6145,\n","         0.4692,  0.1102, -0.7899, -0.6603, -0.3423, -0.6861, -0.9528,  0.4358,\n","        -1.4544, -1.9033, -0.5236,  0.5808,  0.1306, -0.3357,  0.7868,  0.0994,\n","        -0.4945, -0.7406,  0.9727, -0.7283, -1.3570,  0.1255,  0.1410, -0.3538,\n","         0.0936,  0.3550, -0.4863, -0.7971, -0.8238,  0.3398,  0.6045,  0.2673,\n","        -0.2607, -1.4958,  1.1306,  3.7613, -0.8842, -1.2760,  1.1332, -0.5690,\n","         0.8436,  1.8983,  1.3370,  0.1085, -0.5932,  0.6982, -1.1854,  2.3071,\n","         0.1005,  0.9111,  0.7645,  0.7824,  1.5015,  0.5549,  0.3234, -2.5868,\n","         0.0796,  0.5919, -0.0241,  0.3832, -0.5318,  0.7511,  0.4772, -0.0143,\n","        -0.5644, -2.5461, -1.6116, -2.8491,  1.5097, -0.8915,  2.5556,  0.3473,\n","        -0.2836,  0.1034, -1.3498,  1.0184,  0.3205, -0.5310, -1.2338, -0.5728,\n","        -1.3305, -0.2740,  0.7106,  0.6094,  0.0743,  1.0936, -0.3228, -2.7519,\n","         0.2048, -0.5071, -2.1285,  0.1915,  1.0920, -0.4265,  0.4029, -0.2855,\n","         0.4813,  0.4818, -0.5625,  0.7071,  0.8550,  1.2309, -1.5489, -0.8557,\n","        -0.8305, -0.4106,  2.0984, -2.3745,  0.9015,  0.5792, -1.1478, -1.2742,\n","         1.7577,  2.0875,  0.7193, -1.0104,  1.1718, -0.6930,  1.4926,  0.5968,\n","        -0.9770,  1.2665,  0.1805, -0.5647,  1.6443, -1.9981,  0.1515,  0.9078],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 위생\n","tensor([ 4.0818e-01,  7.7013e-01, -1.7981e+00,  1.5412e-01,  4.6696e-01,\n","         1.2207e+00,  1.2350e+00,  9.9089e-01, -8.1319e-01,  1.6296e+00,\n","         1.1583e+00,  4.4364e-01,  1.4523e+00,  1.4666e+00,  1.9024e+00,\n","        -1.0450e+00, -1.4615e-01,  6.1447e-01,  1.5642e-01,  8.6790e-01,\n","         5.6979e-01,  3.4965e-01, -2.8673e-01, -8.3787e-01,  8.5415e-01,\n","        -4.3948e-01,  7.8728e-01, -7.3146e-01, -4.4004e-01, -1.0080e+00,\n","        -1.0435e+00,  1.6556e+00,  7.3176e-01, -6.9701e-01, -2.0615e+00,\n","         1.1869e+00, -2.0587e-01, -2.3387e-01, -1.1445e+00,  7.6852e-01,\n","        -2.8602e-01,  2.1693e+00, -9.7431e-02, -7.2723e-01, -4.6575e-02,\n","        -1.0223e+00,  4.7364e-01,  1.6214e-01, -1.7960e+00, -7.3632e-01,\n","        -1.8843e+00,  8.7097e-01, -7.2857e-01, -1.0291e+00,  1.8079e+00,\n","         2.2921e+00, -1.5413e+00, -8.5764e-01,  1.4824e+00, -1.8613e+00,\n","        -4.5457e-01,  2.4284e+00,  2.7553e+00,  9.8907e-01, -6.7736e-01,\n","         1.6303e+00,  3.4309e-01, -9.2829e-01,  2.0055e-01,  7.3850e-01,\n","         1.7366e+00, -1.5004e-01, -7.4024e-01, -1.5835e+00,  1.1311e-01,\n","         1.3041e+00,  1.1690e+00, -6.3987e-03,  6.3428e-01, -1.5750e+00,\n","         1.7858e-01,  1.3642e+00, -7.9599e-01, -1.5437e+00, -4.4141e-02,\n","        -7.9477e-01,  2.2848e-01, -2.3522e+00,  2.8536e-01,  1.2237e+00,\n","        -2.9822e+00, -7.1096e-01,  2.4569e+00,  5.4127e-01, -1.2296e+00,\n","        -1.1437e-01, -1.3026e-01,  9.6551e-01,  3.1938e-01,  7.1678e-01,\n","         2.1053e-01, -2.7035e-01,  9.7759e-01, -8.0515e-01,  7.0174e-01,\n","         3.3268e-01, -2.1180e+00,  8.9445e-01, -1.5570e-01, -7.2540e-01,\n","         1.3165e+00,  2.0263e-01, -1.9329e+00,  5.4071e-01,  5.0522e-01,\n","         8.4045e-01,  1.3446e+00,  2.5954e-01, -6.9852e-01,  7.8341e-01,\n","         3.9761e-01, -6.4113e-01,  4.9464e-01, -1.0799e+00,  3.3951e-01,\n","        -1.4686e-02,  5.3740e-02,  1.3340e+00, -1.5839e+00,  1.6029e-01,\n","         8.6729e-01,  1.2442e-01,  5.5973e-01, -1.2236e+00,  1.9904e+00,\n","        -2.2153e+00,  1.2721e+00,  1.1699e+00,  4.4494e-01, -1.4492e-02,\n","         1.7813e+00,  5.4536e-01,  1.4836e+00, -1.0438e+00, -3.2419e-01,\n","        -2.9084e-01, -9.3475e-01, -1.0550e-01, -2.7835e-01, -1.8689e+00,\n","         4.8573e-01, -3.5951e-01,  1.0750e+00, -8.8537e-01,  5.5181e-01,\n","         2.5947e-01, -9.7698e-01, -3.0297e-01,  1.1407e+00, -5.9772e-01,\n","        -1.0437e+00,  1.1009e+00, -2.3226e-01, -2.9014e-02,  6.5862e-01,\n","         1.1866e+00,  1.3187e-01,  1.0956e+00, -2.7779e-01, -8.2899e-01,\n","         5.0453e-02,  4.5436e-01,  8.8944e-01,  8.2541e-02,  3.3862e-01,\n","        -1.3985e+00,  5.6335e-01,  1.5988e+00, -1.4891e+00, -1.2376e+00,\n","        -1.6208e+00,  8.4805e-01, -1.1338e+00, -6.0380e-01,  1.8808e-01,\n","         1.6388e-01, -1.5588e+00, -1.6797e-02,  1.8624e+00,  2.1955e+00,\n","         1.9812e-01, -1.2404e+00,  1.1841e-01,  1.5598e+00, -2.9119e-01,\n","        -2.7493e-01, -9.5519e-01, -1.4018e+00,  6.8777e-01, -2.8475e-01,\n","         1.7200e+00, -1.6506e+00, -2.0518e+00, -1.1613e+00, -7.5019e-01,\n","         1.9245e+00, -3.4223e-01,  3.0714e-01,  3.9255e-01,  1.5810e+00,\n","        -2.8254e-01, -4.1773e-01, -2.1188e+00, -2.4500e-03, -1.4602e-01,\n","         9.9199e-01,  6.3660e-01, -1.0430e-01, -2.5904e-01,  1.1897e+00,\n","         2.0146e-01,  2.3405e-01, -3.8022e-03, -2.8803e-01, -2.4262e+00,\n","        -7.1887e-02,  1.4698e+00,  2.5926e-01, -1.7340e+00,  3.7561e-01,\n","        -1.0628e+00, -1.2823e-01, -1.2207e-01,  1.8469e-01, -7.4784e-01,\n","         3.7121e-01, -9.3690e-01,  7.2601e-01,  1.7559e-01,  5.8976e-01,\n","        -1.1840e+00, -1.6198e-01,  1.9414e-01, -4.6721e-01, -9.6447e-01,\n","        -1.9950e+00,  2.1618e+00, -1.2454e+00, -8.2573e-01,  6.8771e-01,\n","        -5.6729e-01,  3.5322e+00, -1.4858e+00, -1.7940e+00,  5.1090e-02,\n","         8.5822e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 가격\n","tensor([-0.6201,  1.8248,  1.0572, -1.4213,  1.2514,  0.4374, -0.4164, -0.2003,\n","        -0.3550,  1.8226,  1.3280,  0.0052,  0.6468,  1.0266,  0.8396, -0.4291,\n","        -1.6214, -1.4187, -0.0550, -0.3232,  0.0352, -1.3888,  1.4145,  0.3347,\n","         0.0942, -0.8522, -0.7029,  0.0693, -0.6872,  0.0155, -0.5048, -0.8466,\n","        -1.3902, -1.5154,  2.2724,  1.1236,  0.3309, -0.7755, -1.7876, -1.5662,\n","         1.7175, -0.7069,  1.9795,  0.3308,  0.6338,  0.6290,  1.4080,  0.8082,\n","        -0.7724, -1.2438, -1.2825, -0.5845,  0.0392, -0.4489, -1.0863,  1.5878,\n","         2.2357, -0.1188, -0.7057, -1.0237, -0.8376,  0.6046, -0.5118, -2.3803,\n","        -1.8227, -0.2228,  0.8650,  0.4046,  0.1620,  0.9078, -0.0418, -0.6507,\n","        -0.3538, -0.1533,  0.5863,  0.2695,  0.0084, -0.2096,  1.2408,  2.0803,\n","        -1.3405,  1.0254,  0.4315, -0.4083, -0.1945, -0.4472,  0.2854, -0.8147,\n","        -0.3372,  0.8179, -1.2152,  0.7416,  0.2158, -0.2052, -0.0789, -0.0743,\n","        -0.0774,  0.2812, -0.8265,  0.3919,  0.0365, -0.2381,  2.9841, -0.5834,\n","        -0.5216, -2.0098, -1.4729, -1.1645,  0.0075, -0.4303,  0.8523, -0.9146,\n","         1.8620,  1.9048,  0.3337,  0.4590, -1.2925,  0.5486,  1.9718, -0.8368,\n","         0.6230, -0.7013, -0.7775, -0.3340, -1.7246,  0.1472, -0.2854,  1.7666,\n","         0.4078,  0.4158, -0.9050, -1.4651,  0.0710, -1.1882, -0.0106, -0.0328,\n","         0.3630,  0.3352, -0.7790, -0.8067, -1.1819,  0.5475,  0.9703, -0.4340,\n","         2.5764,  0.9585, -0.8787,  0.8905, -0.2236, -1.9618, -0.2970,  0.6036,\n","         1.4515,  1.9971, -1.4700, -2.0820, -1.4624,  1.6322, -0.3139, -2.0453,\n","         0.0032, -1.6053,  1.5692, -0.0097, -0.2095,  1.1436, -0.6102,  0.4315,\n","         0.2937, -0.4944,  1.1020, -1.0616, -2.0285,  0.9212, -0.0435,  0.0775,\n","        -0.4005,  0.1596,  0.1931,  0.3354, -0.9405,  0.6266,  1.6048,  0.8429,\n","        -0.9574, -1.4895,  1.9278,  0.5208,  1.1078,  0.1677, -0.5606,  0.2427,\n","         0.4972,  1.3220, -0.7236,  0.5519, -0.7038,  1.2155,  2.7964,  0.0258,\n","         0.7275, -0.4985,  1.7812, -0.1421,  2.4035, -0.8180, -1.2525,  0.0857,\n","         2.3836,  0.3312, -0.3455,  0.2042,  0.1124, -0.0911,  0.3001, -0.0369,\n","        -0.1643,  0.4055,  0.3754, -1.2053, -0.0598, -0.7983,  0.5220, -1.5073,\n","         0.9195,  2.2796, -0.6608, -0.0968,  0.6305, -0.5861, -0.8682,  1.6324,\n","         1.1335, -1.3735, -0.7619, -0.8143,  0.9563, -0.6830,  0.4930,  0.3204,\n","        -1.3672,  0.3734,  0.2349, -0.6295,  0.2012, -1.5519, -0.5066, -0.0324,\n","         0.6750,  0.2522, -0.1400, -0.4180,  1.3016,  0.3703, -0.1599,  0.5314],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_l5cPRZZe-R4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613519628560,"user_tz":-540,"elapsed":588,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"c2f095da-f907-417d-d2ce-e4003cba5e23"},"source":["for word in test_words:\r\n","  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n","  emb = skipgram.embedding(input_id)\r\n","\r\n","  print(f\"Word: {word}\")\r\n","  print(max(emb.squeeze(0)))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Word: 음식\n","tensor(2.5135, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 맛\n","tensor(3.0978, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 서비스\n","tensor(2.1603, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 위생\n","tensor(3.1853, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 가격\n","tensor(2.4698, device='cuda:0', grad_fn=<UnbindBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PmN4IchwisOO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613519685068,"user_tz":-540,"elapsed":54487,"user":{"displayName":"김상현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5jMOH-HgfqkCBt9sIGNROJYqYZMPuvsq-5uqkSw=s64","userId":"12495690518551874965"}},"outputId":"75d1f718-200b-493f-c956-6a4b61ca52f5"},"source":["!apt-get install -qq texlive texlive-xetex texlive-latex-extra pandoc\r\n","!pip install -qq pypandoc\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","\r\n","!jupyter nbconvert --to PDF '/content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb의 사본'"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","Selecting previously unselected package fonts-droid-fallback.\n","(Reading database ... 146425 files and directories currently installed.)\n","Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n","Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n","Selecting previously unselected package fonts-lato.\n","Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n","Unpacking fonts-lato (2.0-2) ...\n","Selecting previously unselected package poppler-data.\n","Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n","Unpacking poppler-data (0.4.8-2) ...\n","Selecting previously unselected package tex-common.\n","Preparing to unpack .../03-tex-common_6.09_all.deb ...\n","Unpacking tex-common (6.09) ...\n","Selecting previously unselected package fonts-lmodern.\n","Preparing to unpack .../04-fonts-lmodern_2.004.5-3_all.deb ...\n","Unpacking fonts-lmodern (2.004.5-3) ...\n","Selecting previously unselected package fonts-noto-mono.\n","Preparing to unpack .../05-fonts-noto-mono_20171026-2_all.deb ...\n","Unpacking fonts-noto-mono (20171026-2) ...\n","Selecting previously unselected package fonts-texgyre.\n","Preparing to unpack .../06-fonts-texgyre_20160520-1_all.deb ...\n","Unpacking fonts-texgyre (20160520-1) ...\n","Selecting previously unselected package javascript-common.\n","Preparing to unpack .../07-javascript-common_11_all.deb ...\n","Unpacking javascript-common (11) ...\n","Selecting previously unselected package libcupsfilters1:amd64.\n","Preparing to unpack .../08-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n","Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n","Selecting previously unselected package libcupsimage2:amd64.\n","Preparing to unpack .../09-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n","Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n","Selecting previously unselected package libijs-0.35:amd64.\n","Preparing to unpack .../10-libijs-0.35_0.35-13_amd64.deb ...\n","Unpacking libijs-0.35:amd64 (0.35-13) ...\n","Selecting previously unselected package libjbig2dec0:amd64.\n","Preparing to unpack .../11-libjbig2dec0_0.13-6_amd64.deb ...\n","Unpacking libjbig2dec0:amd64 (0.13-6) ...\n","Selecting previously unselected package libgs9-common.\n","Preparing to unpack .../12-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n","Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Selecting previously unselected package libgs9:amd64.\n","Preparing to unpack .../13-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n","Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Selecting previously unselected package libjs-jquery.\n","Preparing to unpack .../14-libjs-jquery_3.2.1-1_all.deb ...\n","Unpacking libjs-jquery (3.2.1-1) ...\n","Selecting previously unselected package libkpathsea6:amd64.\n","Preparing to unpack .../15-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libpotrace0.\n","Preparing to unpack .../16-libpotrace0_1.14-2_amd64.deb ...\n","Unpacking libpotrace0 (1.14-2) ...\n","Selecting previously unselected package libptexenc1:amd64.\n","Preparing to unpack .../17-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package rubygems-integration.\n","Preparing to unpack .../18-rubygems-integration_1.11_all.deb ...\n","Unpacking rubygems-integration (1.11) ...\n","Selecting previously unselected package ruby2.5.\n","Preparing to unpack .../19-ruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n","Unpacking ruby2.5 (2.5.1-1ubuntu1.7) ...\n","Selecting previously unselected package ruby.\n","Preparing to unpack .../20-ruby_1%3a2.5.1_amd64.deb ...\n","Unpacking ruby (1:2.5.1) ...\n","Selecting previously unselected package rake.\n","Preparing to unpack .../21-rake_12.3.1-1ubuntu0.1_all.deb ...\n","Unpacking rake (12.3.1-1ubuntu0.1) ...\n","Selecting previously unselected package ruby-did-you-mean.\n","Preparing to unpack .../22-ruby-did-you-mean_1.2.0-2_all.deb ...\n","Unpacking ruby-did-you-mean (1.2.0-2) ...\n","Selecting previously unselected package ruby-minitest.\n","Preparing to unpack .../23-ruby-minitest_5.10.3-1_all.deb ...\n","Unpacking ruby-minitest (5.10.3-1) ...\n","Selecting previously unselected package ruby-net-telnet.\n","Preparing to unpack .../24-ruby-net-telnet_0.1.1-2_all.deb ...\n","Unpacking ruby-net-telnet (0.1.1-2) ...\n","Selecting previously unselected package ruby-power-assert.\n","Preparing to unpack .../25-ruby-power-assert_0.3.0-1_all.deb ...\n","Unpacking ruby-power-assert (0.3.0-1) ...\n","Selecting previously unselected package ruby-test-unit.\n","Preparing to unpack .../26-ruby-test-unit_3.2.5-1_all.deb ...\n","Unpacking ruby-test-unit (3.2.5-1) ...\n","Selecting previously unselected package libruby2.5:amd64.\n","Preparing to unpack .../27-libruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n","Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n","Selecting previously unselected package libsynctex1:amd64.\n","Preparing to unpack .../28-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libtexlua52:amd64.\n","Preparing to unpack .../29-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libtexluajit2:amd64.\n","Preparing to unpack .../30-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libzzip-0-13:amd64.\n","Preparing to unpack .../31-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n","Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n","Selecting previously unselected package lmodern.\n","Preparing to unpack .../32-lmodern_2.004.5-3_all.deb ...\n","Unpacking lmodern (2.004.5-3) ...\n","Selecting previously unselected package preview-latex-style.\n","Preparing to unpack .../33-preview-latex-style_11.91-1ubuntu1_all.deb ...\n","Unpacking preview-latex-style (11.91-1ubuntu1) ...\n","Selecting previously unselected package t1utils.\n","Preparing to unpack .../34-t1utils_1.41-2_amd64.deb ...\n","Unpacking t1utils (1.41-2) ...\n","Selecting previously unselected package tex-gyre.\n","Preparing to unpack .../35-tex-gyre_20160520-1_all.deb ...\n","Unpacking tex-gyre (20160520-1) ...\n","Selecting previously unselected package texlive-binaries.\n","Preparing to unpack .../36-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package texlive-base.\n","Preparing to unpack .../37-texlive-base_2017.20180305-1_all.deb ...\n","Unpacking texlive-base (2017.20180305-1) ...\n","Selecting previously unselected package texlive-fonts-recommended.\n","Preparing to unpack .../38-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n","Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n","Selecting previously unselected package texlive-latex-base.\n","Preparing to unpack .../39-texlive-latex-base_2017.20180305-1_all.deb ...\n","Unpacking texlive-latex-base (2017.20180305-1) ...\n","Selecting previously unselected package texlive-latex-recommended.\n","Preparing to unpack .../40-texlive-latex-recommended_2017.20180305-1_all.deb ...\n","Unpacking texlive-latex-recommended (2017.20180305-1) ...\n","Selecting previously unselected package texlive.\n","Preparing to unpack .../41-texlive_2017.20180305-1_all.deb ...\n","Unpacking texlive (2017.20180305-1) ...\n","Selecting previously unselected package texlive-pictures.\n","Preparing to unpack .../42-texlive-pictures_2017.20180305-1_all.deb ...\n","Unpacking texlive-pictures (2017.20180305-1) ...\n","Selecting previously unselected package texlive-latex-extra.\n","Preparing to unpack .../43-texlive-latex-extra_2017.20180305-2_all.deb ...\n","Unpacking texlive-latex-extra (2017.20180305-2) ...\n","Selecting previously unselected package texlive-plain-generic.\n","Preparing to unpack .../44-texlive-plain-generic_2017.20180305-2_all.deb ...\n","Unpacking texlive-plain-generic (2017.20180305-2) ...\n","Selecting previously unselected package tipa.\n","Preparing to unpack .../45-tipa_2%3a1.3-20_all.deb ...\n","Unpacking tipa (2:1.3-20) ...\n","Selecting previously unselected package texlive-xetex.\n","Preparing to unpack .../46-texlive-xetex_2017.20180305-1_all.deb ...\n","Unpacking texlive-xetex (2017.20180305-1) ...\n","Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up libjs-jquery (3.2.1-1) ...\n","Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n","Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up tex-common (6.09) ...\n","update-language: texlive-base not installed and configured, doing nothing!\n","Setting up poppler-data (0.4.8-2) ...\n","Setting up tex-gyre (20160520-1) ...\n","Setting up preview-latex-style (11.91-1ubuntu1) ...\n","Setting up fonts-texgyre (20160520-1) ...\n","Setting up fonts-noto-mono (20171026-2) ...\n","Setting up fonts-lato (2.0-2) ...\n","Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n","Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n","Setting up libjbig2dec0:amd64 (0.13-6) ...\n","Setting up ruby-did-you-mean (1.2.0-2) ...\n","Setting up t1utils (1.41-2) ...\n","Setting up ruby-net-telnet (0.1.1-2) ...\n","Setting up libijs-0.35:amd64 (0.35-13) ...\n","Setting up rubygems-integration (1.11) ...\n","Setting up libpotrace0 (1.14-2) ...\n","Setting up javascript-common (11) ...\n","Setting up ruby-minitest (5.10.3-1) ...\n","Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n","Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up fonts-lmodern (2.004.5-3) ...\n","Setting up ruby-power-assert (0.3.0-1) ...\n","Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n","update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n","update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n","Setting up texlive-base (2017.20180305-1) ...\n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n","mktexlsr: Updating /var/lib/texmf/ls-R... \n","mktexlsr: Done.\n","tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n","tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n","tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n","tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n","Setting up texlive-fonts-recommended (2017.20180305-1) ...\n","Setting up texlive-plain-generic (2017.20180305-2) ...\n","Setting up texlive-latex-base (2017.20180305-1) ...\n","Setting up lmodern (2.004.5-3) ...\n","Setting up texlive-latex-recommended (2017.20180305-1) ...\n","Setting up texlive-pictures (2017.20180305-1) ...\n","Setting up tipa (2:1.3-20) ...\n","Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n","Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n","update-fmtutil has updated the following file(s):\n","\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n","\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n","If you want to activate the changes in the above file(s),\n","you should run fmtutil-sys or fmtutil.\n","Setting up texlive (2017.20180305-1) ...\n","Setting up texlive-latex-extra (2017.20180305-2) ...\n","Setting up texlive-xetex (2017.20180305-1) ...\n","Setting up ruby2.5 (2.5.1-1ubuntu1.7) ...\n","Setting up ruby (1:2.5.1) ...\n","Setting up ruby-test-unit (3.2.5-1) ...\n","Setting up rake (12.3.1-1ubuntu0.1) ...\n","Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n","Processing triggers for tex-common (6.09) ...\n","Running updmap-sys. This may take some time... done.\n","Running mktexlsr /var/lib/texmf ... done.\n","Building format(s) --all.\n","\tThis may take some time... done.\n","  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[NbConvertApp] Converting notebook /content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb의 사본 to PDF\n","[NbConvertApp] Writing 43496 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 40841 bytes to /content/drive/My Drive/Colab Notebooks/1_naive_bayes.pdf\n"],"name":"stdout"}]}]}